{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification using Conv-LSTM model\n",
    "Here we do hyperparameter grid search by making own GridSearch object and without using library functions or objects (such as GridSearchCV from sklearn). We need to create such an object, because it is not correct to compare neural networks by scores after a fixed number of epochs (due to overfiting and so on) and we need to plot learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Conv1D, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import Callback, ProgbarLogger, BaseLogger\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "from src import data as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../sample_data' #'/home/moskaleona/alenadir/data/rawData' #'C:/Users/alena/Desktop/homed/laba/data/rawData' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dt.DataBuildClassifier(path_to_data).get_data([33], shuffle=True, random_state=1, resample_to=128, windows=[(0.2, 0.5)],baseline_window=(0.2, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = data[33][0], data[33][1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(data[33][0], data[33][1], test_size=0.2, stratify=data[33][1], random_state=108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import logging\n",
    "\n",
    "class LossMetricHistory(Callback):\n",
    "    def __init__(self, n_iter, validation_data=(None,None), verbose=1):\n",
    "        super(LossMetricHistory, self).__init__()\n",
    "        self.n_iter = n_iter\n",
    "        self.x_val, self.y_val = validation_data\n",
    "        if self.x_val is not None and self.y_val is not None:\n",
    "            self.validate = True\n",
    "        else:\n",
    "            self.validate = False\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter(\"%(message)s\")\n",
    "        console.setFormatter(formatter)\n",
    "        if len(self.logger.handlers) > 0:\n",
    "            self.logger.handlers = []\n",
    "        self.logger.addHandler(console)\n",
    "            \n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if self.verbose > 0:\n",
    "            self.logger.info(\"Training began\")\n",
    "            widgets = [\n",
    "                progressbar.Percentage(u'Total progress:%(percentage)3d%%'),\n",
    "                progressbar.Bar(),\n",
    "                #progressbar.DynamicMessage('loss'),\n",
    "                progressbar.ETA(format=u' Time left:  %(eta)8s', format_finished=u' Time: %(elapsed)8s'),\n",
    "                progressbar.SimpleProgress()\n",
    "                ]\n",
    "            bar = progressbar.ProgressBar(max_value=self.n_iter, widgets=widgets)\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accs = [] # accuracy scores\n",
    "        self.val_accs = [] # validation accuracy scores\n",
    "        self.aucs = []# validation ROC AUC scores\n",
    "        self.sens = []# validation sensitivity (or True Positive Rate) scores\n",
    "        self.spc = [] # validation specificity scores\n",
    "        self.thresholds = [] # Decreasing thresholds used to compute specificity and sensitivity\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accs.append(logs.get('acc'))\n",
    "        if self.validate: \n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            self.val_accs.append(logs.get('val_acc'))\n",
    "            self.y_pred = self.model.predict_proba(self.x_val, verbose=0)\n",
    "            self.aucs.append(roc_auc_score(self.y_val, self.y_pred))\n",
    "            \n",
    "            FPR, TPR, thresholds = roc_curve(self.y_val, self.y_pred)\n",
    "            self.sens.append(TPR)\n",
    "            self.spc.append(1-FPR)\n",
    "            self.thresholds.append(thresholds)\n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                self.logger.info(\"Epoch %d/%d: train loss = %.6f, test loss = %.6f\"%(epoch+1, self.n_iter, \n",
    "                                                                    self.losses[-1],self.val_losses[-1]) + \n",
    "                                 \"\\n\\tacc = %.6f, test acc = %.6f\"%(self.accs[-1], self.val_accs[-1]) +\n",
    "                                 \"\\n\\tauc = %.6f\"%(self.aucs[-1]))\n",
    "                bar.update(epoch+1)\n",
    "        elif self.verbose > 0:\n",
    "            self.logger.info(\"Epoch %d/%d results: train loss = %.6f\"%(epoch+1, self.n_iter, self.losses[-1]) + \n",
    "                             \"\\n\\t\\t\\tacc = %.6f\"%(self.accs[-1]))\n",
    "            bar.update(epoch+1)\n",
    "    def on_train_end(self, logs={}):\n",
    "        self.losses = np.array(self.losses)\n",
    "        if self.validate:\n",
    "            self.val_losses = np.array(self.val_losses)\n",
    "            self.scores = {}\n",
    "            self.scores['auc'] = np.array(self.aucs)\n",
    "            self.scores['acc'] = np.array(self.val_accs)\n",
    "            self.scores['sens'] = np.array(self.sens)\n",
    "            self.scores['spc'] = np.array(self.spc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class CnnLstmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, loss='binary_crossentropy', n_filters=10, n_lstm=30, n_iter=150, batch_size=10,\n",
    "                 learning_rate=0.001, l1=0., l2=0.0, dropout=0., dropout_lstm=0., recurrent_dropout=0., threshold=0.5):\n",
    "        self.loss = loss\n",
    "        self.n_lstm = n_lstm\n",
    "        self.n_filters = n_filters\n",
    "        self.n_iter = n_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.dropout = dropout\n",
    "        self.dropout_lstm = dropout_lstm\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def _make_model(self, input_shape):\n",
    "        batch_input_shape = (None, input_shape[1], input_shape[2])\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv1D(self.n_filters, self.kernel_size_, batch_input_shape=batch_input_shape,\n",
    "                         activation='relu', kernel_regularizer=l1_l2(self.l1, self.l2)))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(LSTM(self.n_lstm,\n",
    "                       dropout=self.dropout_lstm, recurrent_dropout=self.recurrent_dropout))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    def _plot_loss(self):\n",
    "        plt.title('Learning curves')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.plot(np.arange(len(self.log_.losses)),self.log_.losses, color='tab:blue', label='train loss')\n",
    "        plt.plot(np.arange(len(self.log_.val_losses)),self.log_.val_losses, color='tab:orange', label='test loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_scores(self,scoring):\n",
    "        plt.title('Validation '+scoring)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel(scoring)\n",
    "        plt.plot(np.arange(len(self.log_.aucs)),self.log_.aucs, color='b')\n",
    "        plt.show()\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, scoring='auc',\n",
    "            verbose=1, plotcurves=False):\n",
    "        # TODO: check the parameters\n",
    "        if verbose > 0:\n",
    "            print(\"Training model with parameters:\", self.get_params())\n",
    "        self.kernel_size_ = X_train.shape[2]\n",
    "        self._make_model(X_train.shape)\n",
    "        self.optimizer_ = RMSprop(lr=self.learning_rate)\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer_, metrics=['acc'])\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            self.log_ = LossMetricHistory(n_iter=self.n_iter, \n",
    "                                          validation_data=(X_val, y_val), verbose=verbose)\n",
    "            self.hist_ = self.model.fit(X_train, y_train,\n",
    "                                        batch_size=self.batch_size,\n",
    "                                        epochs=self.n_iter, validation_data=(X_val, y_val),\n",
    "                                        verbose=0, callbacks=[self.log_])\n",
    "            \n",
    "            self.best_score_ = self.log_.scores[scoring].max()\n",
    "            if plotcurves:\n",
    "                self._plot_loss()\n",
    "                self._plot_scores(scoring)\n",
    "        else:\n",
    "            self.log_ = LossMetricHistory(n_iter=self.n_iter)\n",
    "            self.hist_ = self.model.fit(X_train, y_train,\n",
    "                                        batch_size=self.batch_size,\n",
    "                                        epochs=self.n_iter,\n",
    "                                        verbose=verbose, callbacks=[self.log_])\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        try:\n",
    "            getattr(self, \"kernel_size_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        '''\n",
    "        proba = self.model.predict(X)\n",
    "        return (proba > self.threshold).astype('int32')\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        try:\n",
    "            getattr(self, \"kernel_size_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        '''\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    \n",
    "    def score(self, X, y, scoring='auc'):\n",
    "        try:\n",
    "            if scoring=='auc':\n",
    "                return roc_auc_score(y, self.predict_proba(X))\n",
    "            elif scoring=='acc':\n",
    "                return accuracy_score(y, self.predict(X))\n",
    "            else:\n",
    "                raise ValueError(message=\"No such option: '%s'. Use 'auc' or 'acc'\"%str(scoring))\n",
    "        except ValueError as err:\n",
    "            print(err)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 20, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'progressbar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a2b2af15480a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCnnLstmClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotcurves\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1601d9241060>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val, scoring, verbose, plotcurves)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                         verbose=0, callbacks=[self.log_])\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/moskaleona/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/moskaleona/anaconda3/envs/py27/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     })\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/moskaleona/anaconda3/envs/py27/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3a4752920dd1>\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training began\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             widgets = [\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mprogressbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPercentage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Total progress:%(percentage)3d%%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mprogressbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m#progressbar.DynamicMessage('loss'),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'progressbar' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "clf = CnnLstmClassifier(n_iter=20)\n",
    "clf.fit(X_train, y_train, X_val=X_val, y_val=y_val, verbose=1, plotcurves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.base import clone\n",
    "from functools import reduce\n",
    "\n",
    "class GridSearch:\n",
    "    def __init__(self, estimator, param_grid, scoring='auc',\n",
    "                 cv=None, verbose=0, plot_scores=True, refit=True):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        self.plot_scores = plot_scores\n",
    "        self.refit = refit\n",
    "    \n",
    "    def _get_param_iterator(self):\n",
    "        \"\"\"Return ParameterGrid instance for the given param_grid\"\"\"\n",
    "        return ParameterGrid(self.param_grid)\n",
    "        \n",
    "    def fit(self, X, y, groups=None):\n",
    "        n_splits = cv.get_n_splits(X, y, groups)\n",
    "        candidate_params = list(self._get_param_iterator())\n",
    "        n_candidates = len(candidate_params)\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                  \" {2} fits\".format(n_splits, n_candidates,\n",
    "                                     n_candidates * n_splits))\n",
    "            widgets = [\n",
    "                progressbar.Percentage(u'Total progress:%(percentage)3d%%'),\n",
    "                progressbar.Bar(),\n",
    "                progressbar.DynamicMessage('loss'),\n",
    "                progressbar.ETA(format=u' Time left:  %(eta)8s', format_finished=u' Time: %(elapsed)8s')\n",
    "                ]\n",
    "            bar = progressbar.ProgressBar(max_value=100, widgets=widgets)\n",
    "            bar.update(i, loss=min_so_far)\n",
    "        \n",
    "        self.cv_scores_ = []\n",
    "        for params in candidate_params:\n",
    "            self.cv_scores_.append([])\n",
    "            for train, test in cv.split(X, y, groups):\n",
    "                estimator = clone(self.estimator)\n",
    "                estimator.set_params(**params)\n",
    "                estimator.fit(X[train], y[train], X_val=X[test], y_val=y[test],\n",
    "                              scoring=self.scoring, verbose=self.verbose)\n",
    "                self.cv_scores_[-1].append(estimator.best_score_)\n",
    "        self.cv_scores_ = reduce(lambda a,b: np.vstack((np.array(a),np.array(b))), self.cv_scores_)\n",
    "        self.mean_scores_ = self.cv_scores_.mean(axis=1)\n",
    "        self.best_score_ = self.cv_scores_.max()\n",
    "        self.best_mean_score_ = self.mean_scores_.max()\n",
    "        self.best_ind_ = self.mean_scores_.argmax()\n",
    "        if self.verbose > 0:\n",
    "            print(\"Grid search is almost done.\\n\"\n",
    "                  \"Best score is %.6f, best mean score is %.6f.\\n\"\n",
    "                  \"Now the best estimator is training...\"%(self.best_score_, self.best_mean_score_))\n",
    "        self.best_params_ = candidate_params[self.best_ind_]\n",
    "        if refit:\n",
    "            self.best_estimator_ = clone(self.estimator).set_params(**self.best_params_)\n",
    "            self.best_estimator_.fit(X,y)\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_grid = {\n",
    "    'n_iter' : [100, 200, 300],\n",
    "    'l1' : [0., 0.2, 0.4, 0.6],\n",
    "    'l2' : [0., 0.2, 0.4, 0.6],\n",
    "    'dropout' : [0., 0.2, 0.4, 0.6],\n",
    "    'dropout_lstm' : [0., 0.2, 0.4, 0.6],\n",
    "    'recurrent_dropout' : [0., 0.2, 0.4, 0.6],\n",
    "}\n",
    "'''\n",
    "param_grid = {\n",
    "    'n_iter' : [1,2],\n",
    "    'l1' : [0., 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "model = CnnLstmClassifier()\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size = 0.33, random_state = 108)\n",
    "gs = GridSearch(model, param_grid, cv=cv, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 1, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n",
      "Epoch 1/1: train loss = 0.660301, test loss = 0.667251\n",
      "\tacc = 0.655527, test acc = 0.630208\n",
      "\tauc = 0.528337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 1, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n",
      "Epoch 1/1: train loss = 0.669225, test loss = 0.661522\n",
      "\tacc = 0.586118, test acc = 0.614583\n",
      "\tauc = 0.553552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 1, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n",
      "Epoch 1/1: train loss = 0.714750, test loss = 0.674064\n",
      "\tacc = 0.542416, test acc = 0.588542\n",
      "\tauc = 0.512666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 2, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n",
      "Epoch 1/2: train loss = 0.664372, test loss = 0.645554\n",
      "\tacc = 0.634961, test acc = 0.640625\n",
      "\tauc = 0.574290\n",
      "Epoch 2/2: train loss = 0.643544, test loss = 0.653078\n",
      "\tacc = 0.650386, test acc = 0.630208\n",
      "\tauc = 0.556734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 2, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n",
      "Epoch 1/2: train loss = 0.671275, test loss = 0.670466\n",
      "\tacc = 0.591260, test acc = 0.614583\n",
      "\tauc = 0.508307\n",
      "Epoch 2/2: train loss = 0.645857, test loss = 0.672191\n",
      "\tacc = 0.632391, test acc = 0.619792\n",
      "\tauc = 0.515966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 2, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n",
      "Epoch 1/2: train loss = 0.685627, test loss = 0.699001\n",
      "\tacc = 0.562982, test acc = 0.546875\n",
      "\tauc = 0.452339\n",
      "Epoch 2/2: train loss = 0.630532, test loss = 0.677315\n",
      "\tacc = 0.650386, test acc = 0.614583\n",
      "\tauc = 0.486273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search is almost done.\n",
      "Best score is 0.574290, best mean score is 0.531519.\n",
      "Now the best estimator is training...\n",
      "<type 'dict'> {'n_iter': 1, 'l1': 0.0}\n",
      "('Training model with parameters:', {'loss': 'binary_crossentropy', 'recurrent_dropout': 0.0, 'n_iter': 1, 'learning_rate': 0.001, 'batch_size': 10, 'n_filters': 10, 'l2': 0.0, 'n_lstm': 30, 'l1': 0.0, 'threshold': 0.5, 'dropout_lstm': 0.0, 'dropout': 0.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training began\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "581/581 [==============================] - 7s 12ms/step - loss: 0.6749 - acc: 0.6317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 results: train loss = 0.674933\n",
      "\t\t\tacc = 0.631670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GridSearch instance at 0x7f8c7e7dfd88>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnLstmClassifier(batch_size=10, dropout=0.0, dropout_lstm=0.0, l1=0.0,\n",
      "         l2=0.0, learning_rate=0.001, loss='binary_crossentropy',\n",
      "         n_filters=10, n_iter=1, n_lstm=30, recurrent_dropout=0.0,\n",
      "         threshold=0.5)\n",
      "{'n_iter': 1, 'l1': 0.0}\n",
      "0.5742900907269942\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_estimator_)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressbar examples\n",
    "TODO: add a progressbar to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progressbar test: [=================================]Time:  0:00:0770.0 of 70.0\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "import time\n",
    "bar = progressbar.bar.ProgressBar(maxval=70.0, widgets=[\n",
    "    'progressbar test: ', # Статический текст\n",
    "    progressbar.Bar(left='[', marker='=', right=']'), # Прогресс\n",
    "    progressbar.ETA(),\n",
    "    progressbar.SimpleProgress(), # Надпись \"6 из 10\"\n",
    "]).start()\n",
    "t = 0.0\n",
    "while t <= 70.0:\n",
    "    bar.update(t)\n",
    "    time.sleep(0.01)\n",
    "    t += 0.1\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def dynamic_message():\n",
    "    # Use progressbar.DynamicMessage to keep track of some parameter(s) during\n",
    "    # your calculations\n",
    "    widgets = [\n",
    "        progressbar.Percentage(u'Total progress:%(percentage)3d%%'),\n",
    "        progressbar.Bar(),\n",
    "        progressbar.DynamicMessage('loss'),\n",
    "        progressbar.ETA(format=u' Time left:  %(eta)8s', format_finished=u' Time: %(elapsed)8s')\n",
    "    ]\n",
    "    with progressbar.ProgressBar(max_value=1000, widgets=widgets) as bar:\n",
    "        min_so_far = 1\n",
    "        for i in range(100):\n",
    "            val = random.random()\n",
    "            time.sleep(0.1)\n",
    "            if val < min_so_far:\n",
    "                min_so_far = val\n",
    "            bar.update(i, loss=min_so_far)\n",
    "dynamic_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e246cf372214bd5bd91aec2e7477b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'1st loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8480eb55cc45e0baeed2a3e0044ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbcf8b2e1b046b1a8ad092cb5dac845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31163da78cbe4972869b2a5b3ef17289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a83fba7a0c34407ab1e7607913e8a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b0cdfeb5646f997093b88ebc5c099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117521251ad84667a087824c535d39f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d9ef74c2974cffa9278f735606d801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d5bcde3430405db04d91271f36e973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6486b453353f4c0dbaaa25c1d3835f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c104423e914dddbf39945a028a6232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'2nd loop', max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "\n",
    "for i in tnrange(10, desc='1st loop'):\n",
    "    for j in tqdm_notebook(xrange(10), desc='2nd loop'):\n",
    "        sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 40/200 [00:01<00:04, 39.65it/s]\u001b[A\n",
      " 30%|███       | 60/200 [00:02<00:04, 30.46it/s]\u001b[A\n",
      " 40%|████      | 80/200 [00:03<00:04, 26.22it/s]\u001b[A\n",
      " 50%|█████     | 100/200 [00:04<00:04, 23.89it/s][A\n",
      " 60%|██████    | 120/200 [00:05<00:03, 22.48it/s][A\n",
      " 70%|███████   | 140/200 [00:06<00:02, 21.60it/s][A\n",
      " 80%|████████  | 160/200 [00:07<00:01, 21.00it/s][A\n",
      " 90%|█████████ | 180/200 [00:08<00:00, 20.63it/s][A\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.35it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "pbar1 = tqdm(total=100, position=1)\n",
    "pbar2 = tqdm(total=200, position=0)\n",
    "\n",
    "for i in range(10):\n",
    "    pbar1.update(10)\n",
    "    pbar2.update(20)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
